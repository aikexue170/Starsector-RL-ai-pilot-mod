# Abyssal Redemption (临渊号曙光) - Starsector RL AI Pilot Mod

一个为《远行星号》(Starsector)游戏开发的强化学习AI飞行员模组，使用近端策略优化(PPO)算法训练飞船的移动控制。该项目旨在实现比原版游戏AI更精准的舰船操控，包括移动、避障、停泊等复杂机动。

> **注意**：本项目仍在开发中，尚未达到完全可用的状态。经历了许多失败的尝试，当前采用连续动作空间PPO作为最新方案。

## 功能特性

- **连续动作空间控制**：使用PPO算法在连续动作空间中训练舰船移动，实现平滑的加速、转向和平移控制。
- **实时训练与推理**：通过Socket通信实现Python训练端与Java游戏端的实时数据交换，支持在真实游戏环境中训练和部署。
- **虚拟训练环境**：包含基于Pygame的简易飞船停靠模拟器，可在虚拟环境中快速验证算法。
- **模块化设计**：Java端采用舰船插件(hullmod)形式集成，易于在游戏中启用或禁用AI控制。
- **完整训练工具链**：提供从数据收集、模型训练到游戏内部署的全套工具。
- **技术探索记录**：保留了早期尝试的DQN、LSTM等多种强化学习方案代码，供研究参考。

## 项目结构

```
Abyssal Redemption/
├── data/                    # 游戏数据文件（飞船、武器、系统配置）
├── graphics/               # 美术资源（贴图、特效）
├── jars/                   # 编译后的Java模组及源代码
│   └── src/
│       ├── data/scripts/   # 游戏脚本
│       └── impl/           # 核心实现
│           ├── combat/     # 战斗系统
│           └── hullmods/   # 舰船插件（AI控制核心）
│               ├── PPOTrainer_Continuous.java    # 连续动作PPO训练器
│               ├── ARR_ShipController.java       # 基础舰船控制器
│               └── PPOUser_30state.java          # PPO模型用户端
├── PPO_python_trainer/     # Python训练代码
│   ├── PPO_framework.py           # PPO算法核心实现
│   ├── PPO_train_in_starsector_continuous.py  # 连续动作训练脚本（真实环境）
│   ├── PPO_train_in_virtual_env.py           # 虚拟环境训练脚本
│   ├── PPO_use_in_starsector.py              # 游戏内模型推理
│   ├── PPO_virtual_env.py                    # Pygame虚拟环境
│   ├── replay_buffer.py                      # 经验回放缓冲区
│   ├── models/                              # 训练好的模型文件
│   ├── logs/                                # TensorBoard训练日志
│   └── abandoned_code/                      # 早期尝试方案（DQN、LSTM等）
├── mod_info.json           # 模组元数据
└── LICENSE                 # MIT许可证
```

## 安装与使用

### 前置要求

1. **游戏环境**：《远行星号》0.98a-RC8版本
2. **依赖模组**：MagicLib、LazyLib（必须安装）
3. **Python环境**：Python 3.8+，需要以下包：
   ```bash
   pip install torch numpy pygame tensorboard
   ```

### 安装步骤

1. **安装模组**：将本模组文件夹放入Starsector的`mods/`目录。
2. **启用模组**：在游戏启动器中启用"临渊号曙光"模组。
3. **配置Python环境**：确保Python环境已安装所需依赖。

### 训练AI飞行员

#### 虚拟环境训练（推荐初学者）
```bash
cd PPO_python_trainer
python PPO_train_in_virtual_env.py
```
这将在一个简化的2D环境中训练飞船停靠任务。

#### 真实游戏环境训练
1. **启动游戏**：进入战斗场景，为舰船安装`PPOTrainer_Continuous`插件。
2. **启动训练服务器**：
   ```bash
   cd PPO_python_trainer
   python PPO_train_in_starsector_continuous.py
   ```
3. **开始训练**：游戏中的舰船将通过Socket与Python服务器通信，实时收集数据并训练。

### 使用训练好的模型

1. **训练模型**：完成训练后，模型将保存在`PPO_python_trainer/models/`目录。
2. **部署模型**：使用`PPO_use_in_starsector.py`加载模型并与游戏通信：
   ```bash
   python PPO_use_in_starsector.py --model_path models/ppo_continuous_model_final.pth
   ```
3. **游戏内使用**：为舰船安装`PPOUser_30state`插件，AI将使用训练好的模型进行控制。

## 技术细节

### 状态空间 (11维)
1. 舰船位置 (x, y)
2. 舰船朝向角度
3. 线性速度 (vx, vy)
4. 角速度
5. 战术系统状态（冷却、活跃、空闲）
6. 16个激光传感器数据（用于障碍感知）

### 动作空间 (7维连续动作)
1. **前进/后退**：[-1, 1]，负值表示后退
2. **转向**：[-1, 1]，负值表示左转，正值表示右转
3. **平移**：[-1, 1]，负值表示向左平移，正值表示向右平移
4. **战术系统激活**：[0, 1]，阈值触发
5. **武器组1-3开火**：[0, 1]，连续开火强度

### 奖励函数设计
- **基础移动奖励**：鼓励向目标位置移动
- **速度惩罚**：防止速度过高导致失控
- **角度对齐奖励**：舰船朝向与目标方向一致时获得奖励
- **完成任务奖励**：成功停靠或到达目标区域
- **碰撞惩罚**：与障碍物或其他舰船碰撞

### 算法实现
- **PPO (Proximal Policy Optimization)**：采用Actor-Critic架构，GAE(Generalized Advantage Estimation)计算优势函数
- **网络结构**：共享特征提取层(256维)→Actor/Critic分离头
- **训练参数**：
  - 学习率：1e-4
  - 折扣因子γ：0.98
  - PPO裁剪参数ε：0.1
  - 批量大小：256
  - 训练轮数：8

## 废弃方案

项目早期尝试了多种强化学习方法，均保留在`abandoned_code/`目录中：

### DQN系列 (深度Q网络)
- **基础DQN**：离散动作空间，7个动作
- **LSTM-DQN**：加入长短期记忆网络处理时序依赖
- **高级DQN**：更复杂的网络结构和奖励设计

### 失败原因分析
1. **离散动作空间限制**：舰船控制本质是连续的，离散动作导致控制不流畅
2. **训练稳定性**：DQN系列在复杂环境中难以稳定收敛
3. **实时性要求**：游戏环境需要低延迟响应，部分方案延迟过高

## 开发日志与挑战

### 主要技术挑战
1. **游戏- Python通信**：通过Socket实现实时数据交换，解决同步和延迟问题
2. **状态表示**：设计有效的状态空间捕捉游戏物理特性
3. **奖励塑造**：设计合理的奖励函数引导学习目标行为
4. **训练效率**：在有限的计算资源下提高训练效率

### 里程碑
- **v0.1**：基础Socket通信框架，DQN离散控制
- **v0.2**：引入LSTM处理时序，虚拟训练环境
- **v0.3**：切换到PPO算法，实现连续动作空间
- **v0.4**：优化奖励函数，改进训练稳定性

## 未来计划

1. **多舰船协同**：训练多AI舰船协同作战
2. **复杂任务扩展**：除移动外，增加战斗、巡逻等任务
3. **模型轻量化**：优化模型大小和推理速度
4. **用户友好界面**：提供图形化训练监控和配置界面
5. **社区贡献**：开放训练数据集和基准测试

## 贡献指南

欢迎对本项目做出贡献！你可以通过以下方式参与：

1. **报告问题**：在GitHub Issues中提交bug或建议
2. **改进算法**：尝试不同的RL算法或网络结构
3. **优化性能**：改进代码效率或训练速度
4. **文档完善**：补充使用文档或技术说明

## 许可证

本项目采用MIT许可证。详见[LICENSE](LICENSE)文件。

## 致谢

- **《远行星号》开发团队**：创造了这个优秀的游戏平台
- **MagicLib & LazyLib**：提供了强大的模组开发支持
- **PyTorch社区**：优秀的深度学习框架
- **所有测试参与者**：帮助改进AI行为

---

*"原版AI蠢得要命，我因此萌生出了更精密控制的想法。" —— 项目初心*

> **温馨提示**：强化学习训练需要大量时间和计算资源，请耐心等待。建议先在虚拟环境中验证算法，再在真实游戏环境中训练。